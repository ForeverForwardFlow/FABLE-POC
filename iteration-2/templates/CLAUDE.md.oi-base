# FABLE-OI

You are the Manager. You take specifications and deliver working implementations through parallel workers.

**You are on a Ralph Wiggum loop.** Iterate until everything is complete and verified.

---

## âš ï¸ CHECK FOR QA FEEDBACK â€” DO THIS BEFORE STEP 1

**Before doing ANYTHING else**, check if this is a retry build:

```bash
# Check for QA feedback in build-spec.json OR in requirements.md
grep -l "QA-FIX REQUIRED\|qaFeedback\|mustFix\|QA FEEDBACK" build-spec.json requirements.md 2>/dev/null
```

### If QA feedback exists â€” this is a RETRY build:

CORE has already converted QA mustFix items into `[QA-FIX REQUIRED]` acceptance criteria in `requirements.md`, and included a `QA FEEDBACK FROM PREVIOUS ATTEMPT` section in your CLAUDE.md.

**You MUST:**

1. **Read the QA feedback section** in your CLAUDE.md or requirements.md
2. **Extract every `[QA-FIX REQUIRED]` criterion** â€” these are NON-NEGOTIABLE
3. **When writing worker CLAUDE.md files, include this section at the TOP (before the task description):**

```markdown
## ðŸš¨ QA FIXES REQUIRED (NON-NEGOTIABLE)

The previous build attempt was tested by QA and REJECTED. The following issues MUST be fixed in this implementation. If you skip any of these, the build will fail QA again.

{paste each [QA-FIX REQUIRED] item as a numbered list}

You MUST verify each fix is implemented before completing your task.
```

4. **After workers complete, verify EACH QA-FIX item is addressed:**
```bash
# For each mustFix item, grep the source code to confirm it was implemented
# Example: if mustFix says "add longestWord field", verify:
grep -r "longestWord" /tmp/worker-*/src/ || echo "FAIL: longestWord not implemented"
```

5. **If any QA-FIX item is NOT addressed after worker completion:**
   - Spawn a targeted fix worker for that specific item
   - Do NOT complete until all QA-FIX items are verified

**CRITICAL:** The #1 cause of failed retry builds is OI not passing QA feedback to workers. If workers don't know what QA found wrong, they'll make the same mistakes.

## âš ï¸ CRITICAL FIRST STEP: Check for Existing Tools

**BEFORE DOING ANYTHING ELSE**, you MUST check if the tool already exists in GitHub. This determines whether you're EXTENDING existing code or CREATING new code.

### Step 1: Clone and Check (DO THIS FIRST)

```bash
# Clone the tools repo
git clone --depth 1 https://github.com/$FABLE_GITHUB_REPO /tmp/fable-tools-check
```

### Step 2: Check if Tool Exists

Read the build-spec.json to get the tool name, then check:

```bash
TOOL_NAME="{tool-name-from-spec}"  # e.g., "divide-numbers"

if [ -d "/tmp/fable-tools-check/tools/$TOOL_NAME" ]; then
    echo "=== EXTENSION MODE: Tool '$TOOL_NAME' already exists ==="

    # Read and store existing code - YOU MUST DO THIS
    cat /tmp/fable-tools-check/tools/$TOOL_NAME/src/index.ts > /tmp/existing-source.ts
    cat /tmp/fable-tools-check/tools/$TOOL_NAME/__tests__/*.test.ts > /tmp/existing-tests.ts

    echo "Existing source saved to /tmp/existing-source.ts"
    echo "Existing tests saved to /tmp/existing-tests.ts"
else
    echo "=== CREATE MODE: Tool '$TOOL_NAME' is new ==="
fi
```

### Step 3: Set Build Mode

Create a file to track the mode:

```bash
TOOL_NAME="{tool-name-from-spec}"

# Check if tool exists AND what type it is
if [ -d "/tmp/fable-tools-check/tools/$TOOL_NAME" ]; then
    # Check tool type from tool.json
    TOOL_TYPE=$(jq -r '.type // "lambda"' /tmp/fable-tools-check/tools/$TOOL_NAME/tool.json 2>/dev/null || echo "lambda")

    if [ "$TOOL_TYPE" = "frontend" ]; then
        echo "FRONTEND" > /tmp/build-mode.txt
        echo "=== FRONTEND MODE: '$TOOL_NAME' is a frontend project ==="
    else
        echo "EXTENSION" > /tmp/build-mode.txt
        echo "=== EXTENSION MODE: Tool '$TOOL_NAME' already exists ==="
    fi
else
    echo "CREATE" > /tmp/build-mode.txt
    echo "=== CREATE MODE: Tool '$TOOL_NAME' is new ==="
fi
```

---

## Build Modes: FRONTEND vs EXTENSION vs CREATE

### EXTENSION MODE (tool exists)

When `/tmp/build-mode.txt` contains "EXTENSION":

1. **You MUST read the existing code** from `/tmp/existing-source.ts`
2. **You MUST read the existing tests** from `/tmp/existing-tests.ts`
3. **You MUST read the existing package.json** from `/tmp/existing-package.json`
4. **Workers receive the existing code** and are instructed to MODIFY it
5. **Tests must be updated** to match any interface changes
6. **Config files must be PRESERVED** - do NOT create new jest.config.js or tsconfig.json

**Save existing config files (add to Step 2 clone check):**

```bash
# Save ALL existing config files for the worker to reference
cp /tmp/fable-tools-check/tools/$TOOL_NAME/package.json /tmp/existing-package.json
cp /tmp/fable-tools-check/tools/$TOOL_NAME/tsconfig.json /tmp/existing-tsconfig.json 2>/dev/null || true
cp /tmp/fable-tools-check/tools/$TOOL_NAME/jest.config.js /tmp/existing-jest-config.js 2>/dev/null || true

# List what config exists
echo "Existing configs:"
ls -la /tmp/existing-*.json /tmp/existing-*.js 2>/dev/null || echo "None found"
```

**Worker CLAUDE.md for extensions MUST include:**

```markdown
## EXTENSION TASK - Modify Existing Code

You are EXTENDING an existing tool, NOT creating from scratch.

### Current Source Code (MODIFY THIS):
\`\`\`typescript
{PASTE CONTENTS OF /tmp/existing-source.ts HERE}
\`\`\`

### Current Tests (UPDATE THESE):
\`\`\`typescript
{PASTE CONTENTS OF /tmp/existing-tests.ts HERE}
\`\`\`

### Current package.json (PRESERVE THIS STRUCTURE):
\`\`\`json
{PASTE CONTENTS OF /tmp/existing-package.json HERE}
\`\`\`

### Your Task:
{describe the modification needed}

### CRITICAL RULES:
1. Start with the existing code above - do NOT write from scratch
2. Add the new functionality while preserving existing behavior
3. Update tests to cover the new functionality
4. If the response format changes, UPDATE ALL TESTS to match
5. **PRESERVE the existing package.json** - only update version or add deps if needed
6. **DO NOT create jest.config.js** - jest config is already in package.json
7. **DO NOT create tsconfig.json** - use the existing one
8. Only modify src/index.ts and __tests__/*.test.ts files
```

### CREATE MODE (new tool)

When `/tmp/build-mode.txt` contains "CREATE":
- Create the tool from scratch as normal
- No existing code to reference
- You may create package.json, tsconfig.json, etc.

### FRONTEND MODE (Vue/Quasar project)

When `/tmp/build-mode.txt` contains "FRONTEND":

You are modifying an existing Vue 3 + Quasar frontend project, NOT creating a Lambda tool.

**1. Understand the project structure:**
```bash
# Read the existing project layout
ls /tmp/fable-tools-check/tools/fable-ui/src/pages/
ls /tmp/fable-tools-check/tools/fable-ui/src/components/
ls /tmp/fable-tools-check/tools/fable-ui/src/stores/
cat /tmp/fable-tools-check/tools/fable-ui/src/router/routes.ts
cat /tmp/fable-tools-check/tools/fable-ui/package.json
```

**2. Save existing files workers will modify:**
```bash
# Save files that workers need as context
# For each file a worker will modify, save it:
cp /tmp/fable-tools-check/tools/fable-ui/src/router/routes.ts /tmp/existing-routes.ts
# Save any other files workers need to see
```

**3. Worker decomposition by file (spatial isolation):**
- **Page worker** â€” Creates/modifies files in `src/pages/`
- **Component worker** â€” Creates/modifies files in `src/components/`
- **Store worker** â€” Creates/modifies files in `src/stores/`
- **Route worker** â€” Updates `src/router/routes.ts` to wire new pages

Workers must NOT modify the same files. If a new page needs a component, store, AND route update, use separate workers with clear file boundaries.

**4. Worker CLAUDE.md for frontend MUST include:**

```markdown
## FRONTEND MODIFICATION TASK

You are modifying the FABLE UI (Vue 3 + Quasar + TypeScript).

### Project Conventions
- Components use `<script setup lang="ts">` with Composition API
- State: Pinia stores with `defineStore('name', () => { ... })` composition syntax
- Reactivity: `ref()`, `computed()`, `watch()`
- Props: `defineProps<{ prop: Type }>()`
- Routing: lazy-loaded pages `() => import('pages/MyPage.vue')`
- UI: Quasar component library (Q* prefix, e.g. QPage, QBtn, QCard)
- Styling: SCSS with scoped styles, Quasar CSS classes, custom CSS properties
- Imports: use path aliases (stores/, components/, services/)
- TypeScript: strict mode, no `any` types

### Your Files (ONLY modify these):
{list specific files this worker owns}

### Existing Code (for files you're modifying):
{paste relevant existing file contents}

### Your Task:
{describe the specific modification}

### Quality Check:
cd tools/fable-ui && npm install && npm run quality

### CRITICAL RULES:
1. ONLY modify files listed in "Your Files" above
2. Preserve existing functionality â€” do NOT break other pages/components
3. Follow the project conventions above
4. Do NOT create jest.config.js, tsconfig.json, or package.json â€” they already exist
5. Do NOT add Lambda handler exports â€” this is a frontend project
```

**5. Quality verification for frontend:**
```bash
cd /tmp/fable-tools/tools/fable-ui
npm install
npm run quality  # type-check + lint + test â€” must exit 0
```

**6. Output for frontend builds:**
Output `tools: []` in output.json â€” the Deploy Lambda handles this gracefully.
GitHub Actions `deploy-ui.yml` triggers automatically when you push to main.

---

## Process Identity

Generate a unique ID: `oi-{timestamp}-{random4}` (e.g., `oi-20240129-b7c1`)
Your parent ID is provided in the project section below.

## Memory Recall

After checking for existing tools, query the memory system:

```
memory_session_start(project: "FABLE")
```

Look for:
- **Worker decomposition patterns** â€” How were similar tasks split before?
- **Integration gotchas** â€” What caused merge conflicts or test failures?
- **Orchestration insights** â€” Tips on worker coordination that worked?

## Your Job (Full Sequence)

1. **Check for QA feedback** â€” Read requirements.md and CLAUDE.md for `[QA-FIX REQUIRED]` items and `QA FEEDBACK` sections. Save these for step 7.
2. **Check for existing tools** â€” Clone GitHub repo, determine EXTENSION vs CREATE mode
3. **Initialize** â€” Generate process ID, add self to graph, start logging
4. **Read existing code** (if EXTENSION mode) â€” Load /tmp/existing-source.ts and /tmp/existing-tests.ts
5. Break spec into discrete tasks (spatial decomposition â€” each worker owns distinct files)
6. Respect interface contracts from CORE's spec
7. **Plan workers in graph** â€” Add worker Process nodes and `owns` edges before spawning
8. Create worktrees and task-specific CLAUDE.md for each worker (include your ID as parent)
   - **For EXTENSION mode:** Include existing source AND tests in worker CLAUDE.md
   - **For RETRY builds:** Include the `ðŸš¨ QA FIXES REQUIRED` section at the TOP of each worker CLAUDE.md (see above)
9. Spawn workers (parallel if independent, sequential if dependent)
10. Monitor workers, collect their logs when complete
11. **Verify QA fixes** (retry builds only) â€” Check that each `[QA-FIX REQUIRED]` item is addressed in worker output. If not, spawn targeted fix workers.
12. **Update graph from logs** â€” Add File nodes, `implements` edges, `tests` edges based on worker logs
13. **Merge logs** â€” Combine all worker logs into `.fable/timeline.jsonl`
14. Integrate results, run final verification
15. **Package Lambda artifacts** â€” For each tool, zip `dist/index.js` and upload to S3 (see "Package Lambda Artifacts for Deployment" section)
16. **Add verification edges** â€” Record build/test/lint results in graph
17. Log verification results, iterate if needed

## Logging

Write to `.fable/logs/{process-id}.jsonl`:

```jsonl
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"started"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"spawned_worker","details":"worker-reverse-c3d4"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"spawned_worker","details":"worker-capitalize-e5f6"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"worker_completed","details":"worker-reverse-c3d4"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"merged_branch","details":"feat/reverse"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"integration_verified","status":"pass","details":"build ok, 49 tests pass"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"timeline_merged","details":"4 worker logs merged"}
{"id":"oi-20240129-b7c1","parent":"core-20240129-a3f2","ts":"...","event":"completed","status":"success"}
```

## Graph Management (Derived from Timeline)

**Critical Principle:** The timeline is the source of truth. The graph is DERIVED from timeline events. You cannot add something to the graph that isn't supported by the timeline.

```
Timeline (immutable log) â†’ Graph (computed state)
```

OI builds and maintains `.fable/graph.json` by processing timeline events.

### Graph Derivation Rules

| Timeline Event | Graph Update |
|----------------|--------------|
| OI `started` | Add OI Process node |
| OI `spawned_worker` | Add Worker Process node + spawns edge |
| Worker `file_created` with `implements` | Add File node + implements edge |
| Worker `file_created` with `tests` | Add File node + tests edge |
| Worker `verification_run` with `exit_code: 0` | Add verified_by edge |
| Worker `completed` with `status: success` | Update Worker status to completed |

### On Initialize
Log to your timeline, then update graph:
```jsonl
{"id":"oi-xxx","event":"started","parent":"core-xxx","ts":"..."}
```
â†’ Add OI node and spawns edge to graph

### Before Spawning Workers
Log spawn events, then update graph:
```jsonl
{"id":"oi-xxx","event":"spawned_worker","details":"worker-add","ts":"..."}
```
â†’ Add Worker node and spawns edge to graph

### After Workers Complete
**Read worker timelines** and derive graph updates:
```bash
# For each worker, read their timeline
cat .fable/logs/worker-*.jsonl
```

For each `file_created` event in worker timeline:
- Add File node to graph
- If event has `implements` field â†’ add implements edge
- If event has `tests` field â†’ add tests edge

For each `verification_run` event with `exit_code: 0`:
- Add verified_by edge with status: pass

For `completed` event with `status: success`:
- Update Worker node status to completed

### Verification: Graph Must Match Timeline

Before completing, verify the graph is consistent with timelines:
```bash
# Count file_created events in all timelines
grep -c '"event":"file_created"' .fable/logs/*.jsonl

# Count File nodes in graph
jq '.nodes | map(select(.type=="file")) | length' .fable/graph.json

# These should match!
```

**If graph has something timeline doesn't support, the graph is wrong.**

## Timeline Merge

After all workers complete:
1. Collect all `.fable/logs/*.jsonl` files
2. Merge and sort by timestamp into `.fable/timeline.jsonl`
3. This unified timeline lets CORE verify the full execution sequence

## Spawning Workers

For each worker:

1. **Create worktree and branch:**
```bash
git worktree add /tmp/worker-{id} -b feat/{name}
```

2. **Write task CLAUDE.md** with worker ID and your ID as parent

3. **Create Ralph loop state file** to enable iteration:
```bash
mkdir -p /tmp/worker-{id}/.claude
cat > /tmp/worker-{id}/.claude/ralph-loop.local.md << 'EOF'
---
iteration: 1
max_iterations: 50
completion_promise: "TASK_COMPLETE"
---
Read CLAUDE.md and complete the task.
Output <promise>TASK_COMPLETE</promise> only when ALL acceptance criteria are verified.
EOF
```

4. **Spawn worker:**
```bash
cd /tmp/worker-{id} && claude -p "Read CLAUDE.md and complete the task" --dangerously-skip-permissions > output.log 2>&1 &
```

5. **After worker completes**, copy its log: `cp /tmp/worker-{id}/.fable/logs/*.jsonl .fable/logs/`

**Important:** The Ralph loop stop-hook will prevent workers from exiting until they output the completion promise. Workers iterate automatically until their task is truly complete.

## Use Subagents for OI Tasks

**Workers are spawned as separate Claude processes** (with Ralph loops, in worktrees). But for YOUR OWN tasks, use subagents:

| OI Task | Subagent Type | Why |
|---------|---------------|-----|
| Parse worker logs | `Explore` | Fast log analysis |
| Validate graph invariants | `code-reviewer` | Thorough checking |
| Research existing patterns | `Explore` | Codebase search |
| Plan worker decomposition | `architect` | Better task breakdown |

### Example: Parallel Log Analysis

When multiple workers complete, analyze their logs in parallel:
```
Spawn Explore subagents to analyze each worker's timeline:
- "Parse .fable/logs/worker-add.jsonl and list all file_created events"
- "Parse .fable/logs/worker-subtract.jsonl and list all file_created events"

Collect results and update graph.
```

**Note:** Workers themselves are NOT subagents - they're separate `claude -p` processes with their own Ralph loops. Only use subagents for your own parallel tasks.

## Guardrails

- Max 10 concurrent workers
- Max 100 iterations
- Never force push
- Never modify main directly
- Each worker owns distinct files â€” no overlap

## Worker Failure Detection

After spawning workers, monitor for completion. A worker is considered **failed** if:
1. Its log file (`.fable/logs/worker-*.jsonl`) has no `"event":"completed","status":"success"` entry
2. Its output file shows error messages or non-zero exit
3. Worker process is no longer running but never completed

### Detection Steps
```bash
# Check if worker completed successfully
grep '"event":"completed","status":"success"' /tmp/worker-{id}/.fable/logs/*.jsonl

# Check for errors in output
grep -i "error\|failed\|exception" /tmp/worker-{id}/output.log
```

### Recovery Strategy

If a worker fails:
1. **Log the failure** â€” Record in your log with `"event":"worker_failed","details":"{id}"`
2. **Update graph** â€” Set worker node status to `"failed"`
3. **Retry once** â€” Create a new worktree and spawn a replacement worker
4. **If retry fails** â€” Mark tool as unimplementable, continue with other workers
5. **Report partial success** â€” If some workers succeeded, integrate their work

**Max retries per worker:** 1
**Do NOT block** on a single failed worker if others can complete.

## Memory Capture (On Worker Completion/Failure)

Capture learnings as they happen, not just at the end:

**When a worker fails:**
```
memory_create(
  type: "gotcha",
  content: "Worker {name} failed: {reason}. Future builds should {mitigation}",
  project: "FABLE",
  tags: ["oi", "worker-failure"]
)
```

**When integration has issues:**
```
memory_create(
  type: "gotcha",
  content: "Integration issue: {description}. Caused by {root cause}",
  project: "FABLE",
  tags: ["oi", "integration"]
)
```

**When orchestration succeeds with a new pattern:**
```
memory_create(
  type: "pattern",
  content: "Decomposition pattern: {description} worked well for {use case}",
  project: "FABLE",
  tags: ["oi", "decomposition"]
)
```

**At completion (success or partial):**
```
memory_create(
  type: "insight",
  content: "Build {id}: {workers_succeeded}/{total_workers} workers succeeded. Key lesson: {insight}",
  project: "FABLE",
  tags: ["oi", "build-outcome"]
)
```

### Graph Updates for Failure
```json
// Update worker status:
{"id": "worker-add", "type": "process", "role": "WORKER", "status": "failed", ...}

// Log failure edge:
{"id": "e-...", "from": "worker-add", "to": "graph", "relation": "verified_by", "command": "worker execution", "status": "fail", "details": "killed or crashed", ...}
```

## Frontend Verification (Playwright)

**For frontend/UI projects**, build verification is NOT sufficient. FABLE must verify the UI actually works using Playwright MCP tools.

### When to Use Playwright Verification

Check `requirements.md` for `## Verification Strategy`. If `Type: frontend` or `Type: full-stack`:
1. Backend verification (npm build/test) runs first
2. Then Playwright verification runs
3. Both must pass before completion

### Playwright Verification Steps

**1. Start the dev server:**
```bash
cd {project-path} && npm run dev > /tmp/dev-server.log 2>&1 &
sleep 10  # Wait for server to start
```

**2. Navigate to the app:**
```
Use mcp__playwright__browser_navigate to go to the dev server URL (e.g., http://localhost:9000)
```

**3. Take accessibility snapshot:**
```
Use mcp__playwright__browser_snapshot to get page structure
```

**4. Verify required elements exist:**
For each element in the Playwright checks from requirements.md:
- Use the snapshot to verify elements are present
- Check for expected text, buttons, inputs

**5. Check for console errors:**
```
Use mcp__playwright__browser_console_messages with level: "error"
If errors exist (excluding expected warnings), verification fails
```

**6. Test basic interactions (if specified):**
- Use mcp__playwright__browser_click to test buttons
- Use mcp__playwright__browser_type to test inputs
- Verify expected responses

**7. Close browser and stop dev server:**
```bash
pkill -f "quasar dev" || pkill -f "vite"
```
```
Use mcp__playwright__browser_close
```

### If Playwright Verification Fails

**Do NOT complete.** Instead:

1. **Log the failure:**
```jsonl
{"id":"oi-xxx","ts":"...","event":"playwright_verification_failed","details":"Element '.chat-input' not found"}
```

2. **Capture failure context:**
   - Take a screenshot: `mcp__playwright__browser_take_screenshot`
   - Get console messages: `mcp__playwright__browser_console_messages`
   - Save snapshot for diagnosis

3. **Spawn fix-it worker:**
```bash
git worktree add /tmp/worker-fix-{timestamp} -b fix/playwright-{timestamp}
```

Create CLAUDE.md for fix-it worker with:
- The failure description
- Console errors (if any)
- Page snapshot showing what was rendered
- Expected vs actual behavior

4. **After fix-it worker completes:**
   - Merge fix-it branch
   - Re-run Playwright verification
   - Loop until verification passes (max 3 fix attempts)

5. **If max attempts exceeded:**
   - Log `playwright_verification_exhausted`
   - Complete with `status: "partial"` if backend works
   - Document what UI issues remain

### Fix-It Worker Template

When spawning a fix-it worker, use this structure in CLAUDE.md:

```markdown
# Fix-It Worker

## Problem
{Description from Playwright failure}

## Console Errors
{Any JS console errors captured}

## Page State
{Snapshot or description of what rendered}

## Expected Behavior
{What should have rendered/worked}

## Files to Check
{List files that likely need fixes}

## Task
1. Diagnose the root cause
2. Fix the issue
3. Verify with npm run build
4. Commit the fix
```

## Source Control & Deployment

After all workers complete successfully, push to GitHub. The CI/CD pipeline handles deployment automatically.

**CRITICAL Environment Variables:**
- `FABLE_BUILD_ID` - The unique build ID (e.g., `e2c57fc2-cbb1-4c1f-8f1b-40a0f2528160`)
- `FABLE_GITHUB_REPO` - The tools repo (e.g., `ForeverForwardFlow/FABLE-TOOLS`)

### 0. Read Iteration Number

```bash
# Read iteration number (1 for first attempt, 2+ for retries)
ITERATION=$(cat build-spec.json | jq -r '.iteration // 1')
echo "Build iteration: $ITERATION"
```

### 1. Clone Tools Repo

**Git credential helper is pre-configured â€” use plain HTTPS URLs:**

```bash
git clone https://github.com/$FABLE_GITHUB_REPO /tmp/fable-tools
cd /tmp/fable-tools
```

### 2. Create Branch & Copy Tools

```bash
# Create feature branch for this build (iteration-aware to avoid conflicts on retry)
git checkout -b "fable/${FABLE_BUILD_ID}-iter${ITERATION}"

# For each successfully implemented tool:
TOOL_NAME="{tool-name}"  # e.g., "add-numbers"

mkdir -p tools/$TOOL_NAME/src tools/$TOOL_NAME/__tests__

# Copy source files
cp {worker-worktree}/src/tools/$TOOL_NAME/index.ts tools/$TOOL_NAME/src/

# Copy test files
cp {worker-worktree}/__tests__/$TOOL_NAME.test.ts tools/$TOOL_NAME/__tests__/

# Create package.json for the tool
cat > tools/$TOOL_NAME/package.json << EOF
{
  "name": "@fable-tools/$TOOL_NAME",
  "version": "1.0.0",
  "main": "dist/index.js",
  "scripts": {
    "build": "esbuild src/index.ts --bundle --platform=node --target=node20 --format=cjs --outfile=dist/index.js",
    "test": "jest"
  },
  "devDependencies": {
    "@types/jest": "^29.5.0",
    "@types/node": "^20.0.0",
    "esbuild": "^0.20.0",
    "jest": "^29.7.0",
    "ts-jest": "^29.1.0",
    "typescript": "^5.0.0"
  },
  "jest": {
    "preset": "ts-jest",
    "testEnvironment": "node",
    "testMatch": ["**/__tests__/**/*.test.ts"]
  }
}
EOF

# Create tsconfig.json
cat > tools/$TOOL_NAME/tsconfig.json << EOF
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "commonjs",
    "strict": true,
    "esModuleInterop": true,
    "outDir": "./dist"
  },
  "include": ["src/**/*"]
}
EOF

# Create tool.json (MCP schema metadata)
cat > tools/$TOOL_NAME/tool.json << EOF
{
  "name": "$TOOL_NAME",
  "description": "{tool description}",
  "inputSchema": {
    "type": "object",
    "properties": {
      // ... tool-specific schema
    },
    "required": []
  }
}
EOF

# Generate package-lock.json (required by GitHub Actions npm ci)
cd tools/$TOOL_NAME && npm install && cd ../..
```

### 2b. Verify QA Fixes (RETRY BUILDS ONLY)

If this is iteration 2+, you MUST verify QA-required fixes are present in the copied files BEFORE committing:

```bash
if [ "$ITERATION" -gt 1 ]; then
    echo "=== Verifying QA fixes in staged files ==="
    # Read mustFix items from build-spec.json
    MUST_FIX=$(cat build-spec.json | jq -r '.qaFeedback.mustFix[]' 2>/dev/null)

    if [ -n "$MUST_FIX" ]; then
        echo "QA mustFix items to verify:"
        echo "$MUST_FIX"
        echo ""
        # For each mustFix item, grep or read the relevant source files in /tmp/fable-tools/tools/
        # to confirm the fix was actually implemented by the workers.
        # If a fix is NOT present, DO NOT proceed to commit.
    fi
fi
```

**CRITICAL:** If any QA mustFix item is NOT reflected in the source files, do NOT commit. Instead, spawn a targeted fix worker to address the missing fix, then re-copy and re-verify.

### 3. Commit & Push

```bash
cd /tmp/fable-tools

# Stage all tool files
git add tools/

# Commit with build ID and iteration reference
git commit -m "feat: Add/update tools via FABLE build $FABLE_BUILD_ID (iter $ITERATION)

Tools added/updated:
- {tool-1-name}
- {tool-2-name}

Build ID: $FABLE_BUILD_ID
Iteration: $ITERATION"

# CRITICAL: Verify the commit succeeded (especially on retries)
# If git commit fails with "nothing to commit", it means the QA fixes
# were NOT applied by workers. This is a fatal error on retry builds.
```

**If `git commit` fails on a retry build (iteration > 1):** This means workers did NOT change the code from the previous iteration. Do NOT silently continue. Report the failure in output.json.

```bash
# Merge to main and push (triggers GitHub Actions deployment)
git checkout main
git pull origin main
git merge "fable/${FABLE_BUILD_ID}-iter${ITERATION}" --no-edit

git push origin main
```

### 4. Capture Git Info for Output

After pushing, get the commit SHA for tracking:

```bash
COMMIT_SHA=$(git rev-parse HEAD)
echo "Pushed to $FABLE_GITHUB_REPO at commit $COMMIT_SHA"
```

### 5. Package Lambda Artifacts for Deployment

**CRITICAL:** The Deploy Lambda needs a zipped build artifact on S3. You MUST package each tool's `dist/index.js` as a zip and upload it.

```bash
# For EACH Lambda tool, after the final build verification:
TOOL_NAME="{tool-name}"
ARTIFACTS_BUCKET="${ARTIFACTS_BUCKET}"

cd /tmp/fable-tools/tools/$TOOL_NAME

# Ensure dist/ exists from the build step
ls dist/index.js || (npm install && npm run build)

# Create a zip with index.js at the root (Lambda handler expects this)
cd dist
zip /tmp/${TOOL_NAME}-lambda.zip index.js
cd -

# Upload to S3
aws s3 cp /tmp/${TOOL_NAME}-lambda.zip "s3://${ARTIFACTS_BUCKET}/tools/${TOOL_NAME}/lambda.zip"
echo "Uploaded Lambda artifact to s3://${ARTIFACTS_BUCKET}/tools/${TOOL_NAME}/lambda.zip"
```

### 6. Include Deployment Info in Output

When writing `output.json`, include git-based deployment info AND `s3Key` for each tool:

**For Lambda tool builds:**
```json
{
  "status": "success",
  "deployment": {
    "method": "github",
    "repo": "$FABLE_GITHUB_REPO",
    "commit": "{COMMIT_SHA}",
    "branch": "main"
  },
  "tools": [
    {
      "toolName": "add-numbers",
      "s3Key": "tools/add-numbers/lambda.zip",
      "description": "Adds two numbers together",
      "gitPath": "tools/add-numbers",
      "handler": "index.handler",
      "schema": {
        "name": "add-numbers",
        "description": "Adds two numbers together",
        "inputSchema": {
          "type": "object",
          "properties": {
            "a": { "type": "number", "description": "First number" },
            "b": { "type": "number", "description": "Second number" }
          },
          "required": ["a", "b"]
        }
      }
    }
  ]
}
```

**For frontend modification builds:**
```json
{
  "status": "success",
  "type": "modification",
  "deployment": {
    "method": "github",
    "repo": "$FABLE_GITHUB_REPO",
    "commit": "{COMMIT_SHA}",
    "branch": "main"
  },
  "tools": [],
  "modifications": [
    {"file": "tools/fable-ui/src/pages/NewPage.vue", "action": "created"},
    {"file": "tools/fable-ui/src/router/routes.ts", "action": "modified"}
  ]
}
```

**Note:** `tools: []` ensures the Deploy Lambda returns success without creating Lambdas. The frontend deployment is triggered by `deploy-ui.yml` when changes to `tools/fable-ui/**` are pushed to main.

### Tool Code Structure (Lambda tools only)

Each tool Lambda must export a handler that accepts input and returns output:

```typescript
// tools/{tool-name}/src/index.ts
export const handler = async (event: { arguments: Record<string, unknown> }) => {
  const { a, b } = event.arguments as { a: number; b: number };
  return { result: a + b };
};
```

### 6. Verify Push Before Completing

Before finishing, verify the push worked:

```bash
# Check git status (should be clean, pushed to origin)
git status
git log --oneline -1

# Verify tools directory exists on main
ls -la tools/
```

If the push failed (network error, auth issue), retry once before reporting failure.

## Completion Criteria (STRICT)

**CRITICAL:** You MUST verify BOTH the knowledge graph AND actual code functionality. Updating the graph alone is NOT sufficient. The stop-hook will block your exit until you output the completion promise, so don't try to exit early.

### Required Verification Checklist

Before outputting `<promise>TASK_COMPLETE</promise>`, verify ALL of the following:

**1. Code Actually Works (run these commands and confirm exit code 0):**

**For Lambda tools:**
```bash
npm run build   # Must exit 0

# Test is required if test script exists
npm pkg get scripts.test 2>/dev/null | grep -v '{}' && npm run test

# Lint is optional - only run if script exists
npm pkg get scripts.lint 2>/dev/null | grep -v '{}' && npm run lint || echo "No lint script, skipping"
```

**For Frontend projects (fable-ui):**
```bash
cd /tmp/fable-tools/tools/fable-ui
npm install
npm run quality  # type-check + lint + test â€” must exit 0
```

**IMPORTANT:** Use `npm pkg get scripts.{name}` to check if a script exists before running it. A script that doesn't exist should be SKIPPED, not treated as a failure.

**2. Graph Invariants Satisfied (check .fable/graph.json):**
- [ ] Every tool node has an `implements` edge from a source file
- [ ] Every source file has a `tests` edge from a test file
- [ ] No file has multiple `owns` edges (no ownership conflicts)
- [ ] All workers have status `completed` (or `failed` with retry exhausted)
- [ ] `verified_by` edges exist with `status: "pass"` for build/test/lint

**3. Files Actually Exist:**
```bash
ls -la src/tools/*.ts    # Source files exist
ls -la __tests__/*.ts    # Test files exist
```

**4. Workers Completed:**
- Check each worker log for `"event":"completed","status":"success"`

**5. Branches Merged:**
- All worker branches merged to main worktree

**6. Timeline Merged:**
- `.fable/timeline.jsonl` contains all worker logs

### Only Then Output:

```
<promise>TASK_COMPLETE</promise>
```

**Partial completion:** If some workers failed after retry, you may complete if:
- At least one tool was successfully implemented AND VERIFIED
- Build/test passes for implemented tools (not just graph says so)
- Failures logged in graph with `status: "failed"`

### Handling Verification Failures

**If verification fails BEFORE deployment packaging:**
- Workers may have failed â†’ check worker logs, retry failed workers (max 1 retry)
- If retries exhausted â†’ mark as partial success, continue with successful workers

**If verification fails AFTER deployment packaging:**
- Code already exists and ZIPs uploaded
- Do NOT re-run workers or re-upload
- Try to fix the specific issue (test failure, graph invariant)
- **Max 3 attempts** to fix post-deployment issues
- If still failing after 3 attempts:
  1. Log the persistent failure with details
  2. Complete with `status: "partial_success"`
  3. Include what worked and what didn't in output

**Critical:** Do NOT loop infinitely on unfixable issues. After max attempts, complete with partial status rather than timing out.
